{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies.dp_policies import *\n",
    "from environment_presets import *\n",
    "from environment import RoadEnvironment\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_agent(horizon, max_timestep, seed, iterations_Q, gamma):\n",
    "    return OptimalMDPAgent(horizon, max_timestep, seed, iterations_Q, gamma)\n",
    "\n",
    "def small_environment():\n",
    "    \"\"\"Create a small environment for testing.\"\"\"\n",
    "    env = RoadEnvironment(**small_environment_dict)\n",
    "    return env\n",
    "\n",
    "def test_mdp_policy_agent(horizon, max_timestep, seed, iterations_Q, gamma, env_dict, n_rollouts):\n",
    "    agent = policy_agent(horizon, max_timestep, seed, iterations_Q, gamma)\n",
    "    env = RoadEnvironment(**env_dict)\n",
    "\n",
    "    rewards = []\n",
    "    for rollout in tqdm(range(n_rollouts)):\n",
    "        obs = env.reset()\n",
    "        agent.reset(obs)\n",
    "        done = False\n",
    "        reward_episode = 0\n",
    "        while not done:\n",
    "            state = np.array(env._get_states())\n",
    "            actions = agent.get_action(state)\n",
    "            obs, reward, done, info = env.step(actions)\n",
    "            reward_episode += reward\n",
    "        rewards.append(reward_episode)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:47<00:00, 21.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-533.1 105.27008121968939 -1050.0 -240.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Finite horizon and small env\n",
    "rewards = test_mdp_policy_agent(horizon=\"finite\", max_timestep=50, seed=42, iterations_Q=5000, \n",
    "                                gamma=0.95, env_dict=small_environment_dict, n_rollouts=1000)\n",
    "print(np.mean(rewards), np.std(rewards), np.min(rewards), np.max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 132.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-43.68 31.120051413839278 -150.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Finite horizon and smallest env\n",
    "rewards = test_mdp_policy_agent(horizon=\"finite\", max_timestep=50, seed=42, iterations_Q=5000, \n",
    "                                gamma=0.95, env_dict=smallest_environment_dict, n_rollouts=1000)\n",
    "print(np.mean(rewards), np.std(rewards), np.min(rewards), np.max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:51<00:00, 19.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-551.88 98.33039001244732 -900.0 -240.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Infinite horizon and small env\n",
    "rewards = test_mdp_policy_agent(horizon=\"infinite\", max_timestep=50, seed=42, iterations_Q=5000, \n",
    "                                gamma=0.95, env_dict=small_environment_dict, n_rollouts=1000)\n",
    "print(np.mean(rewards), np.std(rewards), np.min(rewards), np.max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 127.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-43.68 31.120051413839278 -150.0 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Infinite horizon and smallest env\n",
    "rewards = test_mdp_policy_agent(horizon=\"infinite\", max_timestep=50, seed=42, iterations_Q=5000, \n",
    "                                gamma=0.95, env_dict=smallest_environment_dict, n_rollouts=1000)\n",
    "print(np.mean(rewards), np.std(rewards), np.min(rewards), np.max(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
